# ETAPA 2: CRAWLER AUTOMÃTICO

## Contexto
Stage 1 estÃ¡ **completo y en producciÃ³n**. Stage 2 es un salto cualitativo:
- De manual â†’ AutomÃ¡tico
- De Excel hardcodeado â†’ Descubrimiento dinÃ¡mico
- De revisiÃ³n humana â†’ ValidaciÃ³n automatizada

## Objetivo Principal
**Construir un web crawler que descubra automÃ¡ticamente el Ã¡rbol de URLs del sitio.**

### Lo que reemplaza
- âŒ Excel "251028_Ãrbol web - control calidad.xlsx"
- âŒ Script manual `load_sections.py`
- âŒ Lista hardcodeada de 173 URLs
- âœ… Sistema de descubrimiento automÃ¡tico desde URL raÃ­z

---

## Requisitos Funcionales

### 1. Descubrimiento de URLs (Core)
- **Input**: 1 URL raÃ­z (ej: https://www.r4.com)
- **Proceso**: Crawler que sigue enlaces internos recursivamente
- **Output**: Ãrbol de pÃ¡ginas descubiertas â†’ tabla `discovered_urls`
- **Respeta**: robots.txt, rate limiting, dominios permitidos

### 2. ValidaciÃ³n de URLs
- **Detectar enlaces rotos** (404, 500, timeouts)
- **Detectar enlaces incorrectos** (malformados, loops, redirects sospechosos)
- **Crear Ã¡rbol navegable** (parent â†’ children relationships)

### 3. AutomatizaciÃ³n End-to-End
- **Cron job** para ejecutar crawler periÃ³dicamente
- **Sistema de alertas** cuando encuentra problemas
- **EnvÃ­o de emails** automÃ¡tico con resumen
- **Sin intervenciÃ³n manual** una vez configurado

---

## Criterio de Ã‰xito Stage 2
- [ ] Crawler descubre URLs desde raÃ­z automÃ¡ticamente
- [ ] Detecta y reporta enlaces rotos/incorrectos
- [ ] Construye Ã¡rbol de pÃ¡ginas navegable en UI
- [ ] Se ejecuta automÃ¡ticamente (cron/scheduler)
- [ ] EnvÃ­a alertas por email cuando encuentra problemas
- [ ] Reemplaza completamente el flujo manual de Excel

---

## Stack TecnolÃ³gico

### OpciÃ³n A: Requests + BeautifulSoup (ELEGIDA) âœ…
**Pros:**
- RÃ¡pido, ligero
- FÃ¡cil de debuggear
- Ya usamos requests en el proyecto
- Suficiente para HTML estÃ¡tico

**Contras:**
- No ejecuta JavaScript
- Limitado para SPAs (Single Page Apps)

**DecisiÃ³n**: Empezar con esta opciÃ³n. Reevaluar si el sitio requiere JavaScript.

### OpciÃ³n B: Scrapy (Descartada para MVP)
**Por quÃ© descartada:**
- Curva de aprendizaje innecesaria
- MÃ¡s dependencias
- Overkill para empezar

**Reconsiderar si**: Necesitamos crawlear mÃºltiples sitios en paralelo

### OpciÃ³n C: Playwright (PROHIBIDO Stage 2)
**Por quÃ© prohibido:**
- Necesita navegador headless (pesado)
- MÃ¡s lento
- Complejidad innecesaria para Stage 2

**Solo considerar en Stage 3**: Si el sitio absolutamente requiere JavaScript

---

## Arquitectura Propuesta

### Nuevas Tablas BD

#### `discovered_urls`
```sql
CREATE TABLE discovered_urls (
    id SERIAL PRIMARY KEY,
    url TEXT UNIQUE NOT NULL,
    parent_url_id INTEGER REFERENCES discovered_urls(id),
    depth INTEGER NOT NULL DEFAULT 0,  -- Nivel en el Ã¡rbol (0=raÃ­z)
    discovered_at TIMESTAMP DEFAULT NOW(),
    last_checked TIMESTAMP,
    status_code INTEGER,  -- HTTP status (200, 404, etc)
    response_time FLOAT,  -- Milisegundos
    is_broken BOOLEAN DEFAULT FALSE,
    error_message TEXT,
    active BOOLEAN DEFAULT TRUE,
    crawl_run_id INTEGER REFERENCES crawl_runs(id)
);

CREATE INDEX idx_discovered_urls_parent ON discovered_urls(parent_url_id);
CREATE INDEX idx_discovered_urls_broken ON discovered_urls(is_broken);
CREATE INDEX idx_discovered_urls_crawl_run ON discovered_urls(crawl_run_id);
```

#### `crawl_runs`
```sql
CREATE TABLE crawl_runs (
    id SERIAL PRIMARY KEY,
    started_at TIMESTAMP DEFAULT NOW(),
    finished_at TIMESTAMP,
    status TEXT CHECK (status IN ('running', 'completed', 'failed', 'cancelled')),
    root_url TEXT NOT NULL,
    max_depth INTEGER DEFAULT 5,
    urls_discovered INTEGER DEFAULT 0,
    urls_broken INTEGER DEFAULT 0,
    urls_timeout INTEGER DEFAULT 0,
    errors TEXT,
    created_by TEXT  -- user_name que iniciÃ³ el crawl
);

CREATE INDEX idx_crawl_runs_status ON crawl_runs(status);
CREATE INDEX idx_crawl_runs_started ON crawl_runs(started_at DESC);
```

#### `url_changes`
```sql
CREATE TABLE url_changes (
    id SERIAL PRIMARY KEY,
    url_id INTEGER REFERENCES discovered_urls(id) ON DELETE CASCADE,
    change_type TEXT CHECK (change_type IN ('new', 'broken', 'fixed', 'removed', 'status_change')),
    old_value TEXT,
    new_value TEXT,
    detected_at TIMESTAMP DEFAULT NOW(),
    details TEXT
);

CREATE INDEX idx_url_changes_url_id ON url_changes(url_id);
CREATE INDEX idx_url_changes_type ON url_changes(change_type);
CREATE INDEX idx_url_changes_detected ON url_changes(detected_at DESC);
```

---

### Estructura de Archivos (Stage 2)

```
agendaRenta4/
â”œâ”€â”€ app.py                       # Flask app (Stage 1 + nuevas rutas crawler)
â”œâ”€â”€ utils.py                     # Utilidades compartidas
â”œâ”€â”€ constants.py                 # Constantes
â”œâ”€â”€ crawler/                     # ğŸ†• MÃ³dulo crawler (Stage 2)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ crawler.py               # LÃ³gica principal de crawling
â”‚   â”œâ”€â”€ validator.py             # ValidaciÃ³n de enlaces
â”‚   â”œâ”€â”€ scheduler.py             # Tareas periÃ³dicas (APScheduler)
â”‚   â””â”€â”€ config.py                # ConfiguraciÃ³n especÃ­fica del crawler
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ base.html
â”‚   â”œâ”€â”€ inicio.html
â”‚   â”œâ”€â”€ ...                      # Templates Stage 1
â”‚   â”œâ”€â”€ crawler/                 # ğŸ†• Templates crawler (Stage 2)
â”‚   â”‚   â”œâ”€â”€ dashboard.html       # Vista del Ã¡rbol de URLs
â”‚   â”‚   â””â”€â”€ results.html         # Resultados de Ãºltima ejecuciÃ³n
â”‚   â””â”€â”€ emails/
â”‚       â”œâ”€â”€ alert_notification.html  # Stage 1
â”‚       â””â”€â”€ crawler_report.html      # ğŸ†• Email resumen crawler (Stage 2)
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css            # Estilos compartidos
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ crawler_tree.js      # ğŸ†• JS para Ã¡rbol navegable (Stage 2)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_tasks_for_period.py
â”‚   â”œâ”€â”€ load_sections.py         # âš ï¸ Deprecar en Stage 2
â”‚   â””â”€â”€ run_crawler.py           # ğŸ†• Script para cron (Stage 2)
â””â”€â”€ migrations/
    â””â”€â”€ 002_add_crawler_tables.sql  # ğŸ†• SQL para crear tablas (Stage 2)
```

**Total archivos nuevos Stage 2**: ~10 archivos
**Total archivos proyecto**: 25-30 archivos (sigue siendo manejable)

---

## Reglas de Stage 2

### âœ… PERMITIDO en Stage 2

#### Estructura y CÃ³digo
- Crear mÃ³dulo `crawler/` separado (4-5 archivos)
- Nuevas tablas en BD (3 tablas + Ã­ndices)
- Nuevas rutas en app.py (<10 rutas):
  - `/crawler` - Dashboard
  - `/crawler/start` - Iniciar crawl manual
  - `/crawler/status/<id>` - Estado de crawl
  - `/crawler/results/<id>` - Resultados detallados
  - `/crawler/tree` - Vista de Ã¡rbol
- Background jobs con APScheduler
- Nuevas dependencias: `requests`, `beautifulsoup4`, `apscheduler`

#### Complejidad Permitida
- **Clases** si hay estado complejo:
  - `Crawler` class con estado (queue, visited, stats)
  - `URLValidator` class con configuraciÃ³n
- **Separar responsabilidades**:
  - crawler.py â†’ descubrimiento
  - validator.py â†’ validaciÃ³n
  - scheduler.py â†’ automatizaciÃ³n
- **Config especÃ­fico**: crawler/config.py para settings del crawler

#### Abstracciones Justificadas
- **Queue-based crawling** â†’ manejar 1000+ URLs sin recursiÃ³n infinita
- **Visitor pattern** â†’ marcar URLs visitadas
- **Rate limiter** â†’ no saturar servidor

### âŒ PROHIBIDO en Stage 2

#### NO Sobre-ingenierÃ­a
- âŒ Refactorizar app.py en blueprints (solo si app.py > 1,500 lÃ­neas)
- âŒ Repository pattern para BD (queries directos funcionan)
- âŒ Factory pattern para crawler (solo 1 implementaciÃ³n)
- âŒ Interfaces/ABC sin 2+ implementaciones reales
- âŒ Dependency injection framework (YAGNI)

#### NO Scope Creep
- âŒ Machine Learning / detecciÃ³n inteligente de problemas
- âŒ Navegador headless (Playwright/Selenium)
- âŒ ComparaciÃ³n de contenido HTML (Stage 3)
- âŒ Performance monitoring avanzado (Stage 3)
- âŒ Sistema de usuarios/permisos avanzado (mantener Stage 1)
- âŒ Multi-tenancy / mÃºltiples clientes

#### NO Optimizaciones Prematuras
- âŒ ParalelizaciÃ³n con threads/async (empezar sÃ­ncrono)
- âŒ CachÃ© de requests (empezar sin cachÃ©)
- âŒ Base de datos separada para crawler (usar PostgreSQL existente)
- âŒ Microservicios (mantener monolito)

---

## Fases de ImplementaciÃ³n

### Fase 2.1: Crawler BÃ¡sico (MVP) ğŸ¯
**DuraciÃ³n estimada**: 2-3 sesiones
**Objetivo**: Probar concepto con lÃ­mite de 50 URLs

**Tareas:**
- [ ] Crear tabla `discovered_urls` y `crawl_runs`
- [ ] Script `crawler.py` con crawling bÃ¡sico:
  - Fetch URL con requests
  - Parse HTML con BeautifulSoup
  - Extraer enlaces `<a href="...">`
  - Seguir solo enlaces internos (mismo dominio)
  - Guardar en BD con parent_url_id
- [ ] **LÃ­mite**: 50 URLs mÃ¡ximo para pruebas
- [ ] Respetar rate limiting: 1 request/segundo
- [ ] Timeout: 10 segundos por request
- [ ] Ruta `/crawler/start` para iniciar crawl manual
- [ ] Ruta `/crawler/results` para ver URLs descubiertas (tabla simple)

**Criterio de Ã©xito Fase 2.1:**
- âœ… Descubre 50 URLs de prueba desde URL raÃ­z
- âœ… Las muestra en UI (tabla simple)
- âœ… Respeta rate limiting y timeout
- âœ… No crashea con enlaces malformados

**NO hacer en Fase 2.1:**
- âŒ DetecciÃ³n de enlaces rotos (Fase 2.2)
- âŒ Ãrbol navegable (Fase 2.4)
- âŒ AutomatizaciÃ³n (Fase 2.3)
- âŒ Emails (Fase 2.3)

---

### Fase 2.2: ValidaciÃ³n y DetecciÃ³n ğŸ”
**DuraciÃ³n estimada**: 2 sesiones
**Objetivo**: Detectar problemas en las URLs descubiertas

**Tareas:**
- [ ] `validator.py` para validar cada URL:
  - Verificar status_code (200=OK, 404=broken, 500=error)
  - Medir response_time
  - Detectar redirects (301, 302)
  - Detectar timeouts
- [ ] Actualizar `discovered_urls` con:
  - `status_code`
  - `is_broken`
  - `error_message`
- [ ] Crear tabla `url_changes` para histÃ³rico
- [ ] Marcar cambios: 'new', 'broken', 'fixed'
- [ ] Ruta `/crawler/broken` para ver solo enlaces rotos
- [ ] Badge en sidebar: "Enlaces rotos: 5"

**Criterio de Ã©xito Fase 2.2:**
- âœ… Identifica enlaces rotos en las 50 URLs de prueba
- âœ… Muestra solo enlaces rotos en UI
- âœ… Guarda histÃ³rico de cambios
- âœ… Badge en sidebar actualizado

**NO hacer en Fase 2.2:**
- âŒ ValidaciÃ³n avanzada (loops, redirects sospechosos) â†’ opcional
- âŒ Performance monitoring â†’ Stage 3

---

### Fase 2.3: Escalar y Automatizar ğŸ¤–
**DuraciÃ³n estimada**: 2 sesiones
**Objetivo**: Crawl completo + automatizaciÃ³n

**Tareas:**
- [ ] **Quitar lÃ­mite de 50 URLs** â†’ crawl completo
- [ ] AÃ±adir max_depth configurable (default: 5 niveles)
- [ ] Script `run_crawler.py` para ejecutar desde cron:
  ```python
  #!/usr/bin/env python3
  from crawler.crawler import Crawler
  from crawler.config import CRAWLER_CONFIG

  crawler = Crawler(CRAWLER_CONFIG)
  crawler.run()
  ```
- [ ] Configurar APScheduler en app.py:
  - Ejecutar crawler 1x/dÃ­a a las 3:00 AM
  - Job en background (no bloquear Flask)
- [ ] Sistema de alertas:
  - Si encuentra >10 enlaces rotos â†’ crear alerta
  - Si encuentra >50 nuevas URLs â†’ notificar
- [ ] Template `emails/crawler_report.html`
- [ ] EnvÃ­o automÃ¡tico de email con resumen:
  - URLs descubiertas
  - Enlaces rotos encontrados
  - Tiempo de ejecuciÃ³n
- [ ] Ruta `/crawler/schedule` para configurar frecuencia

**Criterio de Ã©xito Fase 2.3:**
- âœ… Crawler corre automÃ¡ticamente 1x/dÃ­a
- âœ… No requiere intervenciÃ³n manual
- âœ… EnvÃ­a email con resumen
- âœ… Crea alertas cuando encuentra problemas

**ConfiguraciÃ³n cron alternativa (si APScheduler no funciona):**
```bash
# Ejecutar crawler diariamente a las 3:00 AM
0 3 * * * cd /home/user/agendaRenta4 && /usr/bin/python3 scripts/run_crawler.py
```

---

### Fase 2.4: Ãrbol Navegable UI ğŸŒ³
**DuraciÃ³n estimada**: 1-2 sesiones
**Objetivo**: UI bonita para explorar Ã¡rbol de URLs

**Tareas:**
- [ ] Template `crawler/dashboard.html` con Ã¡rbol navegable
- [ ] JavaScript `crawler_tree.js` para:
  - Renderizar Ã¡rbol jerÃ¡rquico (parent/children)
  - Expandir/colapsar nodos
  - Resaltar enlaces rotos (rojo)
  - Mostrar depth, status_code en hover
- [ ] Filtros en UI:
  - "Solo enlaces rotos"
  - "Por profundidad (depth)"
  - "Por dominio"
- [ ] BÃºsqueda: buscar URL en Ã¡rbol
- [ ] BotÃ³n "Re-crawl Now" para forzar ejecuciÃ³n manual
- [ ] Exportar Ã¡rbol a JSON/CSV

**Criterio de Ã©xito Fase 2.4:**
- âœ… Ãrbol navegable visualmente atractivo
- âœ… FÃ¡cil encontrar enlaces rotos
- âœ… Filtros funcionan
- âœ… BotÃ³n re-crawl funciona

**InspiraciÃ³n UI:**
- Tree view colapsable (como explorador de archivos)
- Usar librerÃ­a JS: d3.js, jsTree, o simple recursiÃ³n HTML

---

## IntegraciÃ³n con Stage 1

### OpciÃ³n A: Convivencia Temporal (RECOMENDADA para Stage 2)
- Tabla `sections` **mantiene** las 173 URLs actuales (Stage 1)
- Tabla `discovered_urls` es **paralela** (solo lectura Stage 2)
- Stage 1 sigue funcionando igual
- En Stage 3: MigraciÃ³n gradual sections â†’ discovered_urls

**Pros:**
- Stage 1 no se rompe
- Podemos comparar manual vs automÃ¡tico
- TransiciÃ³n gradual

**Contras:**
- 2 fuentes de verdad temporalmente

### OpciÃ³n B: Reemplazo Completo (Stage 3)
- Deprecar tabla `sections`
- Migrar datos sections â†’ discovered_urls
- Todas las tareas usan discovered_urls
- Eliminar load_sections.py

**DecisiÃ³n**: Empezar con OpciÃ³n A en Stage 2, evaluar OpciÃ³n B en Stage 3

---

## ConfiguraciÃ³n del Crawler

### crawler/config.py
```python
CRAWLER_CONFIG = {
    # URLs
    'root_url': 'https://www.r4.com',
    'allowed_domains': ['www.r4.com', 'r4.com'],

    # LÃ­mites
    'max_depth': 5,
    'max_urls': None,  # None = sin lÃ­mite
    'timeout': 10,  # segundos

    # Rate limiting
    'delay_between_requests': 1.0,  # segundos
    'max_retries': 3,

    # Comportamiento
    'follow_redirects': True,
    'respect_robots_txt': True,
    'user_agent': 'AgendaRenta4-Crawler/2.0 (monitoring)',

    # Filtros
    'ignore_patterns': [
        r'/static/',
        r'/media/',
        r'\.pdf$',
        r'\.jpg$',
        r'\.png$',
    ],

    # Alertas
    'alert_threshold_broken_links': 10,
    'alert_threshold_new_urls': 50,
}
```

---

## Rendimiento y LÃ­mites

### Rate Limiting
- **1 request/segundo** (configurable)
- Sleep entre requests: `time.sleep(delay)`
- Respetar `Retry-After` header si servidor pide espera

### Timeouts
- **10 segundos** por request (configurable)
- Si timeout â†’ marcar como error, no crashear crawler

### Max Depth
- **5 niveles** por defecto (configurable)
- Nivel 0: URL raÃ­z
- Nivel 1: Enlaces desde raÃ­z
- Nivel 2: Enlaces desde nivel 1
- etc.

### Max URLs
- **Sin lÃ­mite** en producciÃ³n (Fase 2.3+)
- **50 URLs** en Fase 2.1 (MVP testing)
- Configurable en `CRAWLER_CONFIG`

### Memoria
- **No cargar todo en memoria** â†’ usar BD como queue
- Procesar URL â†’ guardar â†’ marcar como visitado
- Queue: SELECT urls WHERE status_code IS NULL LIMIT 100

---

## Seguridad

### Respetar robots.txt
```python
from urllib.robotparser import RobotFileParser

rp = RobotFileParser()
rp.set_url(f"{root_url}/robots.txt")
rp.read()

if rp.can_fetch(user_agent, url):
    # Crawl permitido
else:
    # Skip URL
```

### User-Agent Identificable
```python
headers = {
    'User-Agent': 'AgendaRenta4-Crawler/2.0 (monitoring; +https://ejemplo.com/crawler-info)'
}
```

### Solo Dominio Permitido
```python
from urllib.parse import urlparse

def is_allowed_domain(url, allowed_domains):
    domain = urlparse(url).netloc
    return domain in allowed_domains
```

### AutenticaciÃ³n (si el sitio requiere login)
```python
session = requests.Session()
session.post(login_url, data={'user': 'X', 'pass': 'Y'})
# Usar session.get() en lugar de requests.get()
```

**Nota**: Si el sitio requiere autenticaciÃ³n compleja, evaluar si crawler es viable

---

## Monitoreo y Logs

### Logs Detallados
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/crawler.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger('crawler')

logger.info(f"Crawling URL: {url}")
logger.warning(f"Broken link: {url} (status={status_code})")
logger.error(f"Timeout: {url}")
```

### Tabla crawl_runs (HistÃ³rico)
- Guardar cada ejecuciÃ³n con stats:
  - URLs descubiertas
  - Enlaces rotos
  - Tiempo de ejecuciÃ³n
- Permite anÃ¡lisis histÃ³rico:
  - "Â¿CuÃ¡ntos enlaces rotos habÃ­a hace 1 mes?"
  - "Â¿CuÃ¡nto ha crecido el sitio?"

### Dashboard en Tiempo Real
- Ruta `/crawler/live` (opcional Fase 2.4)
- Mostrar crawler corriendo en vivo:
  - URLs procesadas / total
  - Velocidad (URLs/min)
  - Enlaces rotos encontrados
  - Progreso: barra de progreso
- WebSocket para updates en tiempo real (opcional)

---

## Anti-Patterns a Evitar

### âŒ Crawler Recursivo Infinito
**Problema:**
```python
def crawl(url, depth=0):
    links = get_links(url)
    for link in links:
        crawl(link, depth+1)  # Stack overflow!
```

**SoluciÃ³n:**
```python
queue = [root_url]
visited = set()

while queue:
    url = queue.pop(0)
    if url in visited:
        continue

    visited.add(url)
    links = get_links(url)
    queue.extend(links)
```

### âŒ Requests Sin Timeout
**Problema:**
```python
response = requests.get(url)  # Puede colgar forever
```

**SoluciÃ³n:**
```python
response = requests.get(url, timeout=10)
```

### âŒ Ignorar Rate Limiting
**Problema:**
```python
for url in urls:
    fetch(url)  # Spam al servidor!
```

**SoluciÃ³n:**
```python
for url in urls:
    fetch(url)
    time.sleep(1)  # Esperar 1 segundo
```

### âŒ Crawlear Sitios Externos
**Problema:**
```python
for link in all_links:
    crawl(link)  # Puede crawlear Google, Facebook, etc
```

**SoluciÃ³n:**
```python
for link in all_links:
    if is_allowed_domain(link, allowed_domains):
        crawl(link)
```

### âŒ Procesar Todo en Memoria
**Problema:**
```python
all_urls = []  # Lista de 10,000 URLs en RAM
```

**SoluciÃ³n:**
```python
# Usar BD como queue
cursor.execute("SELECT url FROM discovered_urls WHERE status_code IS NULL LIMIT 100")
```

### âŒ Mezclar LÃ³gica Crawler con app.py
**Problema:**
- Poner crawler.py dentro de app.py (spaghetti code)

**SoluciÃ³n:**
- MÃ³dulo `crawler/` separado
- app.py solo tiene rutas que llaman al crawler

---

## Salida de Stage 2

**Cambiar a Stage 3 cuando:**
- âœ… Crawler funciona 100% automÃ¡tico en producciÃ³n
- âœ… Detecta y reporta problemas sin fallos
- âœ… Has usado el sistema durante **2+ semanas**
- âœ… Tienes feedback real sobre quÃ© mejorar
- âœ… Stage 1 y Stage 2 conviven estable

**Stage 3 Tentativo (basado en feedback Stage 2):**
- ComparaciÃ³n de contenido entre versiones
- DetecciÃ³n de cambios en elementos especÃ­ficos HTML
- Performance monitoring (tiempos de carga)
- MigraciÃ³n completa: deprecar tabla `sections`
- O lo que Stage 2 revele como necesario

---

## Criterio de Calidad Stage 2

### Preguntas Clave (responder SÃ para considerar Stage 2 completo):

1. **Funcionalidad**
   - Â¿El crawler descubre URLs automÃ¡ticamente sin intervenciÃ³n manual? â†’ SÃ
   - Â¿Detecta enlaces rotos y reporta en UI? â†’ SÃ
   - Â¿Se ejecuta automÃ¡ticamente (cron/scheduler)? â†’ SÃ
   - Â¿EnvÃ­a alertas por email cuando hay problemas? â†’ SÃ

2. **Robustez**
   - Â¿Maneja timeouts sin crashear? â†’ SÃ
   - Â¿Respeta rate limiting? â†’ SÃ
   - Â¿Evita loops infinitos? â†’ SÃ
   - Â¿Logs detallados para debugging? â†’ SÃ

3. **Usabilidad**
   - Â¿Puedo ver el Ã¡rbol de URLs fÃ¡cilmente? â†’ SÃ
   - Â¿Puedo filtrar solo enlaces rotos? â†’ SÃ
   - Â¿Puedo forzar re-crawl manual? â†’ SÃ
   - Â¿El dashboard es intuitivo? â†’ SÃ

4. **Mantenibilidad**
   - Â¿El cÃ³digo estÃ¡ en mÃ³dulo separado (`crawler/`)? â†’ SÃ
   - Â¿Stage 1 sigue funcionando sin cambios? â†’ SÃ
   - Â¿Puedo agregar validaciones nuevas fÃ¡cilmente? â†’ SÃ
   - Â¿La configuraciÃ³n estÃ¡ centralizada? â†’ SÃ

Si todas las respuestas son SÃ â†’ **Stage 2 completado** âœ…

---

## Notas Finales

### FilosofÃ­a Stage 2
- **Prioridad**: Funcionalidad sobre perfecciÃ³n
- **IteraciÃ³n**: MVP rÃ¡pido â†’ mejorar basado en uso real
- **Pragmatismo**: Soluciones simples sobre arquitecturas complejas
- **Evidencia**: Solo aÃ±adir complejidad si hay dolor real

### Cuando Dudar
Si te encuentras pensando:
- "Â¿DeberÃ­a abstraer esto?"
- "Â¿Necesito una interfaz aquÃ­?"
- "Â¿Esto debe ser una clase?"

**Responde:**
1. Â¿Resuelve un problema real (no teÃ³rico)?
2. Â¿Hace el cÃ³digo mÃ¡s fÃ¡cil de entender?
3. Â¿Tengo 2+ casos de uso concretos?

Si las 3 respuestas son NO â†’ **No lo hagas** (YAGNI)

### Recuerda
- Stage 1 funciona y estÃ¡ en producciÃ³n
- Stage 2 no debe romper Stage 1
- Simple > Perfecto
- CÃ³digo que funciona > CÃ³digo "elegante"

---

**Ã‰xito del proyecto = Stage 1 + Stage 2 funcionando en producciÃ³n, NO arquitectura perfecta**
