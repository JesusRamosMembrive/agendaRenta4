# Stage 2: Implementation Plan - Web Crawler AutomÃ¡tico

## TL;DR

**Objetivo**: Reemplazar la gestiÃ³n manual de URLs (Excel) con un sistema automÃ¡tico de descubrimiento y validaciÃ³n.

**DuraciÃ³n estimada**: 7-9 sesiones (~10-15 horas)

**Stack**: Requests + BeautifulSoup + APScheduler + PostgreSQL

---

## Fases de ImplementaciÃ³n

### Fase 2.1: Crawler MVP (2-3 sesiones) ğŸ¯
**Objetivo**: Probar concepto con 50 URLs

**Entregables:**
- âœ… Crawler bÃ¡sico que descubre URLs desde raÃ­z
- âœ… Tabla `discovered_urls` con relaciones parent/child
- âœ… UI simple: lista de URLs descubiertas
- âœ… Rate limiting: 1 request/segundo
- âœ… Timeout: 10 segundos

**Criterio de Ã©xito:**
- Descubre 50 URLs de prueba
- No crashea con enlaces malformados
- Respeta rate limiting

**NO hacer:**
- âŒ DetecciÃ³n de enlaces rotos
- âŒ Ãrbol navegable
- âŒ AutomatizaciÃ³n

---

### Fase 2.2: ValidaciÃ³n (2 sesiones) ğŸ”
**Objetivo**: Detectar enlaces rotos

**Entregables:**
- âœ… `validator.py`: detectar 404, 500, timeouts
- âœ… Actualizar `discovered_urls`: status_code, is_broken
- âœ… Tabla `url_changes`: histÃ³rico de cambios
- âœ… Ruta `/crawler/broken`: filtro de enlaces rotos
- âœ… Badge sidebar: "Enlaces rotos: X"

**Criterio de Ã©xito:**
- Identifica enlaces rotos correctamente
- Guarda histÃ³rico de cambios
- Badge actualizado en tiempo real

---

### Fase 2.3: AutomatizaciÃ³n (2 sesiones) ğŸ¤–
**Objetivo**: Crawl completo + cron + emails

**Entregables:**
- âœ… Quitar lÃ­mite de 50 URLs
- âœ… `run_crawler.py`: script standalone para cron
- âœ… APScheduler: ejecutar 1x/dÃ­a automÃ¡ticamente
- âœ… Sistema de alertas: >10 enlaces rotos â†’ alerta
- âœ… Template `emails/crawler_report.html`
- âœ… EnvÃ­o automÃ¡tico de email con resumen

**Criterio de Ã©xito:**
- Crawler corre 1x/dÃ­a sin intervenciÃ³n
- Email enviado correctamente
- Alertas generadas cuando hay problemas

---

### Fase 2.4: UI Ãrbol (1-2 sesiones) ğŸŒ³
**Objetivo**: Dashboard visual atractivo

**Entregables:**
- âœ… Template `crawler/dashboard.html`: Ã¡rbol navegable
- âœ… JavaScript: expand/collapse, resaltar rotos
- âœ… Filtros: solo rotos, por depth, por dominio
- âœ… BÃºsqueda: encontrar URL en Ã¡rbol
- âœ… BotÃ³n "Re-crawl Now"
- âœ… Exportar Ã¡rbol a JSON/CSV

**Criterio de Ã©xito:**
- Ãrbol visualmente atractivo
- FÃ¡cil encontrar enlaces rotos
- Filtros funcionan correctamente

---

## Arquitectura Simplificada

### Nuevas Tablas BD
```sql
-- URLs descubiertas por crawler
discovered_urls (id, url, parent_url_id, depth, status_code, is_broken, ...)

-- HistÃ³rico de ejecuciones
crawl_runs (id, started_at, finished_at, urls_discovered, urls_broken, ...)

-- Cambios detectados
url_changes (id, url_id, change_type, detected_at, ...)
```

### Nuevos Archivos (~10 archivos)
```
crawler/
â”œâ”€â”€ crawler.py      # LÃ³gica de crawling (queue, visited, fetch)
â”œâ”€â”€ validator.py    # ValidaciÃ³n (status codes, timeouts)
â”œâ”€â”€ scheduler.py    # APScheduler config
â””â”€â”€ config.py       # Settings (root_url, max_depth, rate limit)

templates/crawler/
â”œâ”€â”€ dashboard.html  # UI Ã¡rbol navegable
â””â”€â”€ results.html    # Lista simple de resultados

templates/emails/
â””â”€â”€ crawler_report.html  # Email resumen

scripts/
â””â”€â”€ run_crawler.py  # Script para cron job

migrations/
â””â”€â”€ 002_add_crawler_tables.sql  # CREATE TABLE statements
```

---

## Principios Stage 2

### âœ… PERMITIDO
- MÃ³dulo `crawler/` separado (responsabilidad clara)
- Clases con estado (ej: Crawler con queue/visited)
- Nuevas dependencias: requests, beautifulsoup4, apscheduler
- <10 rutas nuevas en app.py

### âŒ PROHIBIDO
- Refactorizar app.py (solo si >1,500 lÃ­neas)
- Navegador headless (Playwright/Selenium)
- Machine Learning / IA
- ComparaciÃ³n de contenido HTML (Stage 3)
- Optimizaciones prematuras (async, cache)

---

## MÃ©tricas de Ã‰xito

**Fase 2.1:**
- [ ] 50 URLs descubiertas desde raÃ­z

**Fase 2.2:**
- [ ] Enlaces rotos identificados correctamente

**Fase 2.3:**
- [ ] Crawler corre automÃ¡ticamente 1x/dÃ­a
- [ ] Email enviado sin fallos

**Fase 2.4:**
- [ ] Ãrbol navegable fÃ¡cil de usar
- [ ] Filtros funcionan

**Stage 2 Completo:**
- [ ] Reemplaza flujo manual del Excel
- [ ] Stage 1 sigue funcionando sin cambios
- [ ] 0 crashes en producciÃ³n durante 1 semana

---

## Siguiente Paso

**Leer antes de empezar Fase 2.1:**
- `.claude/02-stage2-rules.md` - Arquitectura detallada y reglas
- `.claude/00-project-brief.md` - Contexto del proyecto

**Comando inicial:**
```bash
# Crear estructura bÃ¡sica
mkdir -p crawler templates/crawler templates/emails scripts migrations
touch crawler/__init__.py crawler/crawler.py crawler/config.py
```

---

## Recursos

**DocumentaciÃ³n:**
- [Requests Docs](https://requests.readthedocs.io/)
- [BeautifulSoup Docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [APScheduler Docs](https://apscheduler.readthedocs.io/)

**Anti-Patterns a Evitar:**
- âŒ Crawler recursivo infinito â†’ usar queue + visited set
- âŒ Requests sin timeout â†’ siempre timeout=10
- âŒ Ignorar rate limiting â†’ sleep(1) entre requests
- âŒ Crawlear sitios externos â†’ validar dominio siempre
- âŒ Procesar todo en memoria â†’ usar BD como queue

---

**Ãšltima actualizaciÃ³n**: 2025-10-30
**PrÃ³ximo paso**: Fase 2.1 - Crawler MVP con lÃ­mite de 50 URLs
