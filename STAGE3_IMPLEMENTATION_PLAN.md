# Stage 3 Implementation Plan
## Automatizaci√≥n de 8 Tareas de Calidad Web

**Fecha de inicio**: 2025-10-31
**Timeline estimado**: 3-4 meses
**Presupuesto**: Solo herramientas gratuitas
**Objetivo**: Minimizar el trabajo manual en 173 URLs con 8 tipos de verificaciones

---

## üìã Contexto del Proyecto

**Situaci√≥n actual**:
- Tu esposa usa un Excel para marcar manualmente 8 tareas en 173 URLs
- Stage 1 (Manual): Sistema de gesti√≥n de tareas ‚úÖ
- Stage 2 (Crawler): Descubrimiento autom√°tico de URLs y enlaces rotos ‚úÖ (~90% completado)

**Objetivo Stage 3**:
Automatizar las 8 tareas para que el sistema las ejecute y solo notifique cuando requieran atenci√≥n humana.

**Tareas a automatizar**:
1. ‚úÖ Enlaces rotos (~90% completado en Stage 2)
2. ‚úÖ Enlaces incorrectos (~90% completado en Stage 2)
3. üî≤ Im√°genes (alt text, tama√±o, optimizaci√≥n, rotas)
4. üî≤ CTAs (call-to-action buttons)
5. üî≤ Textos - erratas (spelling/grammar)
6. üî≤ Informaci√≥n actualizada (content changes)
7. üî≤ FAQ (frequently asked questions)
8. üî≤ Dise√±o (accessibility, contrast, responsive)

---

## üéØ Estrategia General

### Principios de Dise√±o
1. **Incremental**: Implementar tarea por tarea, no todas a la vez
2. **Validaci√≥n temprana**: Probar cada tarea en 10-20 URLs antes de escalar a 173
3. **Notificaci√≥n inteligente**: Solo alertar cuando haya problemas reales
4. **Falsos positivos m√≠nimos**: Mejor no detectar algo que inundar con falsos positivos
5. **Portfolio-ready**: C√≥digo limpio, documentado, demostrable

### Arquitectura Propuesta
```
app.py (actual: 1,647 l√≠neas)
    ‚Üì
calidad/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ enlaces.py         # Ya existe (Stage 2)
    ‚îú‚îÄ‚îÄ imagenes.py        # NUEVO - Phase 3.1
    ‚îú‚îÄ‚îÄ ctas.py            # NUEVO - Phase 3.2
    ‚îú‚îÄ‚îÄ textos.py          # NUEVO - Phase 3.3
    ‚îú‚îÄ‚îÄ contenido.py       # NUEVO - Phase 3.4
    ‚îú‚îÄ‚îÄ faq.py             # NUEVO - Phase 3.5
    ‚îú‚îÄ‚îÄ diseno.py          # NUEVO - Phase 3.6
    ‚îî‚îÄ‚îÄ base.py            # Utilidades comunes

templates/calidad/
    ‚îú‚îÄ‚îÄ dashboard.html     # Dashboard unificado con 8 checks
    ‚îú‚îÄ‚îÄ imagenes.html
    ‚îú‚îÄ‚îÄ ctas.html
    ‚îú‚îÄ‚îÄ textos.html
    ‚îú‚îÄ‚îÄ contenido.html
    ‚îú‚îÄ‚îÄ faq.html
    ‚îî‚îÄ‚îÄ diseno.html
```

---

## üìÖ Fases de Implementaci√≥n

### ‚úÖ Phase 3.0: Preparaci√≥n (ANTES de empezar)
**Duraci√≥n estimada**: 1 semana
**Objetivo**: Refactorizar crawler existente y preparar arquitectura modular

**Tareas**:
- [ ] Mover l√≥gica del crawler de `app.py` a `crawler/engine.py`
- [ ] Crear m√≥dulo base `calidad/base.py` con clase `QualityCheck`
- [ ] Actualizar base de datos con tabla `quality_checks`:
```sql
CREATE TABLE quality_checks (
    id SERIAL PRIMARY KEY,
    url_id INTEGER REFERENCES discovered_urls(id),
    check_type VARCHAR(50) NOT NULL, -- 'enlaces', 'imagenes', 'ctas', etc.
    status VARCHAR(20) NOT NULL,     -- 'pass', 'fail', 'warning', 'pending'
    severity VARCHAR(20),             -- 'critical', 'high', 'medium', 'low'
    details JSONB,                    -- Detalles espec√≠ficos del check
    checked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(url_id, check_type, checked_at)
);

CREATE INDEX idx_quality_checks_url ON quality_checks(url_id);
CREATE INDEX idx_quality_checks_type ON quality_checks(check_type);
CREATE INDEX idx_quality_checks_status ON quality_checks(status);
```

**Resultado esperado**:
- C√≥digo m√°s modular y mantenible
- Base lista para agregar nuevos checks sin modificar crawler base
- Reducir `app.py` de 1,647 l√≠neas a ~1,000 l√≠neas

---

### üî≤ Phase 3.1: Verificaci√≥n de Im√°genes (PRIORIDAD 1)
**Duraci√≥n estimada**: 2 semanas
**Valor para el usuario**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Muy alto - tarea muy repetitiva manualmente)

#### ¬øQu√© detectar?
1. **Im√°genes rotas** (404, timeout)
2. **Im√°genes sin alt text** (accesibilidad)
3. **Alt text vac√≠o o gen√©rico** ("image", "img123", etc.)
4. **Im√°genes muy pesadas** (>500KB para web, >200KB para m√≥vil)
5. **Im√°genes sin lazy loading** (performance)
6. **Formatos no optimizados** (usar WebP cuando sea posible)
7. **Dimensiones incorrectas** (imagen 4000x3000 mostrada en 300x200)

#### Herramientas gratuitas
- **BeautifulSoup4** (ya instalado) - parsear `<img>` tags
- **Pillow** - analizar dimensiones, formato, tama√±o
- **requests** - verificar si imagen carga correctamente
- **imagehash** (opcional) - detectar im√°genes duplicadas

#### Implementaci√≥n
```python
# calidad/imagenes.py
import requests
from PIL import Image
from io import BytesIO
from bs4 import BeautifulSoup

class ImageChecker:
    """Verificador de calidad de im√°genes"""

    MAX_FILE_SIZE_WEB = 500 * 1024  # 500KB
    MAX_FILE_SIZE_MOBILE = 200 * 1024  # 200KB

    GENERIC_ALT_TEXTS = [
        'image', 'img', 'photo', 'picture',
        'imagen', 'foto', 'banner'
    ]

    def check_url(self, url, html_content):
        """Analiza todas las im√°genes de una URL"""
        soup = BeautifulSoup(html_content, 'html.parser')
        images = soup.find_all('img')

        results = {
            'total_images': len(images),
            'issues': [],
            'warnings': [],
            'passed': []
        }

        for idx, img in enumerate(images):
            img_result = self._check_single_image(img, url, idx)

            if img_result['severity'] == 'critical':
                results['issues'].append(img_result)
            elif img_result['severity'] == 'warning':
                results['warnings'].append(img_result)
            else:
                results['passed'].append(img_result)

        return results

    def _check_single_image(self, img_tag, base_url, index):
        """Verifica una imagen individual"""
        src = img_tag.get('src', '')
        alt = img_tag.get('alt', '')
        loading = img_tag.get('loading', '')

        issues = []

        # 1. Verificar alt text
        if not alt:
            issues.append("Missing alt text (accessibility issue)")
        elif alt.lower().strip() in self.GENERIC_ALT_TEXTS:
            issues.append(f"Generic alt text: '{alt}'")

        # 2. Verificar lazy loading
        if loading != 'lazy' and index > 2:  # Las primeras 2-3 im√°genes no necesitan lazy
            issues.append("Missing lazy loading attribute")

        # 3. Verificar si imagen carga
        if src:
            img_url = self._resolve_url(src, base_url)
            try:
                response = requests.get(img_url, timeout=5)
                if response.status_code != 200:
                    issues.append(f"Broken image: HTTP {response.status_code}")
                else:
                    # 4. Verificar tama√±o del archivo
                    file_size = len(response.content)
                    if file_size > self.MAX_FILE_SIZE_WEB:
                        issues.append(f"Large file size: {file_size/1024:.0f}KB")

                    # 5. Verificar dimensiones vs display size
                    img_obj = Image.open(BytesIO(response.content))
                    width_attr = img_tag.get('width', '')
                    height_attr = img_tag.get('height', '')

                    if width_attr and height_attr:
                        display_w = int(width_attr.replace('px', ''))
                        display_h = int(height_attr.replace('px', ''))

                        # Si la imagen real es >2x el tama√±o de display, est√° mal optimizada
                        if img_obj.width > display_w * 2 or img_obj.height > display_h * 2:
                            issues.append(
                                f"Over-sized: {img_obj.width}x{img_obj.height} "
                                f"displayed as {display_w}x{display_h}"
                            )

                    # 6. Verificar formato
                    if img_obj.format in ['PNG', 'BMP'] and file_size > 100*1024:
                        issues.append(f"Consider WebP format instead of {img_obj.format}")

            except requests.Timeout:
                issues.append("Image load timeout (>5s)")
            except Exception as e:
                issues.append(f"Error loading image: {str(e)}")
        else:
            issues.append("Missing src attribute")

        return {
            'src': src,
            'alt': alt,
            'issues': issues,
            'severity': 'critical' if any('broken' in i.lower() for i in issues) else 'warning' if issues else 'pass'
        }

    def _resolve_url(self, img_src, base_url):
        """Convierte URL relativa en absoluta"""
        from urllib.parse import urljoin
        return urljoin(base_url, img_src)
```

#### Base de datos
```sql
-- Agregar columna en discovered_urls
ALTER TABLE discovered_urls
ADD COLUMN image_check_status VARCHAR(20) DEFAULT 'pending',
ADD COLUMN image_issues JSONB;

-- O usar la tabla quality_checks (preferido)
INSERT INTO quality_checks (url_id, check_type, status, details)
VALUES (123, 'imagenes', 'fail', '{"broken": 2, "missing_alt": 5}'::jsonb);
```

#### UI - Dashboard de Im√°genes
- **Vista resumen**: "X im√°genes rotas, Y sin alt text, Z muy pesadas"
- **Vista detallada**: Lista de URLs con problemas de im√°genes
- **Filtros**: Por tipo de issue, por severidad
- **Acciones**: "Marcar como revisado", "Ignorar warning"

#### Testing incremental
1. Probar en 10 URLs primero
2. Revisar falsos positivos manualmente
3. Ajustar umbrales (tama√±os, alt text gen√©ricos, etc.)
4. Escalar a 50 URLs
5. Finalmente a las 173 URLs

**Esfuerzo**: ‚≠ê‚≠ê‚≠ê (Medio - herramientas disponibles)
**Valor**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Muy alto - tarea muy manual actualmente)

---

### üî≤ Phase 3.2: Verificaci√≥n de CTAs (PRIORIDAD 2)
**Duraci√≥n estimada**: 1.5 semanas
**Valor para el usuario**: ‚≠ê‚≠ê‚≠ê‚≠ê (Alto - cr√≠tico para conversi√≥n)

#### ¬øQu√© detectar?
1. **CTAs ausentes** en p√°ginas importantes (homepage, servicios, contacto)
2. **CTAs rotos** (enlaces 404)
3. **CTAs ocultos** (display:none, visibility:hidden)
4. **CTAs con mal contraste** (texto no legible)
5. **CTAs sin texto descriptivo** ("Click aqu√≠" vs "Solicitar cotizaci√≥n")
6. **CTAs duplicados** (mismo texto, mismo destino, m√∫ltiples veces)

#### Herramientas gratuitas
- **BeautifulSoup4** - parsear botones y enlaces
- **cssselect** o **lxml** - evaluar CSS de los elementos
- **Regex** - detectar patrones de texto en CTAs

#### Implementaci√≥n
```python
# calidad/ctas.py
from bs4 import BeautifulSoup
import re

class CTAChecker:
    """Verificador de Call-to-Action buttons"""

    # Patrones de CTAs comunes
    CTA_PATTERNS = {
        'button_tags': ['button', 'a'],
        'button_classes': ['btn', 'button', 'cta', 'call-to-action'],
        'button_roles': ['button'],
    }

    # Textos gen√©ricos que deben evitarse
    GENERIC_CTA_TEXTS = [
        'click aqu√≠', 'click here', 'm√°s', 'more',
        'leer m√°s', 'read more', 'ver m√°s', 'see more',
        'aqu√≠', 'here'
    ]

    # URLs donde esperamos encontrar CTAs
    IMPORTANT_PAGES = [
        '/', '/home', '/index',
        '/servicios', '/services',
        '/contacto', '/contact',
        '/productos', '/products'
    ]

    def check_url(self, url, html_content):
        """Analiza CTAs en una URL"""
        soup = BeautifulSoup(html_content, 'html.parser')

        # Detectar todos los CTAs
        ctas = self._find_ctas(soup)

        results = {
            'total_ctas': len(ctas),
            'issues': [],
            'warnings': [],
            'passed': []
        }

        # Si es p√°gina importante y no tiene CTAs
        if self._is_important_page(url) and len(ctas) == 0:
            results['issues'].append({
                'type': 'missing_cta',
                'message': 'Important page has no CTAs',
                'severity': 'critical'
            })

        # Analizar cada CTA
        seen_ctas = {}
        for cta in ctas:
            cta_result = self._check_single_cta(cta)

            # Detectar duplicados
            cta_key = f"{cta_result['text']}|{cta_result['href']}"
            if cta_key in seen_ctas:
                cta_result['issues'].append('Duplicate CTA')
            seen_ctas[cta_key] = True

            if cta_result['severity'] == 'critical':
                results['issues'].append(cta_result)
            elif cta_result['severity'] == 'warning':
                results['warnings'].append(cta_result)
            else:
                results['passed'].append(cta_result)

        return results

    def _find_ctas(self, soup):
        """Encuentra todos los CTAs en el HTML"""
        ctas = []

        # Buscar por tags
        for tag in self.CTA_PATTERNS['button_tags']:
            elements = soup.find_all(tag)
            for el in elements:
                # Filtrar: debe ser bot√≥n o link prominente
                if self._looks_like_cta(el):
                    ctas.append(el)

        return ctas

    def _looks_like_cta(self, element):
        """Determina si un elemento parece un CTA"""
        # Verificar clases
        classes = element.get('class', [])
        for cls in classes:
            if any(pattern in cls.lower() for pattern in self.CTA_PATTERNS['button_classes']):
                return True

        # Verificar role
        if element.get('role') in self.CTA_PATTERNS['button_roles']:
            return True

        # Links con texto corto y llamativo (heur√≠stica)
        if element.name == 'a':
            text = element.get_text(strip=True)
            # CTAs t√≠picos: 2-4 palabras, con verbos de acci√≥n
            words = text.split()
            if 2 <= len(words) <= 5:
                action_verbs = ['comprar', 'solicitar', 'descargar', 'ver', 'contactar',
                                'buy', 'request', 'download', 'view', 'contact']
                if any(verb in text.lower() for verb in action_verbs):
                    return True

        return False

    def _check_single_cta(self, cta):
        """Verifica un CTA individual"""
        text = cta.get_text(strip=True)
        href = cta.get('href', '')

        issues = []

        # 1. Verificar texto gen√©rico
        if text.lower() in self.GENERIC_CTA_TEXTS:
            issues.append(f"Generic CTA text: '{text}'")

        # 2. Verificar que tenga destino
        if not href or href == '#':
            issues.append("CTA with no destination")

        # 3. Verificar si est√° oculto (CSS check - simplificado)
        style = cta.get('style', '')
        if 'display:none' in style or 'visibility:hidden' in style:
            issues.append("Hidden CTA")

        # 4. Verificar longitud del texto
        if len(text) < 3:
            issues.append(f"CTA text too short: '{text}'")

        return {
            'text': text,
            'href': href,
            'issues': issues,
            'severity': 'critical' if any('no destination' in i for i in issues) else 'warning' if issues else 'pass'
        }

    def _is_important_page(self, url):
        """Determina si la URL es una p√°gina importante"""
        from urllib.parse import urlparse
        path = urlparse(url).path.rstrip('/')
        return path in self.IMPORTANT_PAGES or path == ''
```

#### UI - Dashboard de CTAs
- **Vista resumen**: "X CTAs rotos, Y CTAs gen√©ricos, Z p√°ginas sin CTAs"
- **Vista por p√°gina**: Mostrar CTAs encontrados en cada p√°gina
- **Heatmap**: P√°ginas importantes vs n√∫mero de CTAs

**Esfuerzo**: ‚≠ê‚≠ê‚≠ê (Medio - requiere heur√≠sticas)
**Valor**: ‚≠ê‚≠ê‚≠ê‚≠ê (Alto - impacta conversi√≥n)

---

### üî≤ Phase 3.3: Verificaci√≥n de Textos y Erratas (PRIORIDAD 3)
**Duraci√≥n estimada**: 2 semanas
**Valor para el usuario**: ‚≠ê‚≠ê‚≠ê‚≠ê (Alto - profesionalismo)

#### ¬øQu√© detectar?
1. **Errores ortogr√°ficos** (typos)
2. **Errores gramaticales b√°sicos**
3. **Texto en MAY√öSCULAS** (mala pr√°ctica)
4. **Texto muy largo sin p√°rrafos** (legibilidad)
5. **Palabras repetidas** ("el el", "la la")
6. **Espacios m√∫ltiples** o caracteres raros

#### Herramientas gratuitas
- **language-tool-python** (LanguageTool API gratuita, 20 peticiones/minuto)
- **pyspellchecker** (offline, diccionario espa√±ol/ingl√©s)
- **Regex** - detectar patrones problem√°ticos

#### Implementaci√≥n
```python
# calidad/textos.py
from spellchecker import SpellChecker
import re
from bs4 import BeautifulSoup

class TextChecker:
    """Verificador de calidad de textos"""

    def __init__(self, language='es'):
        self.spell = SpellChecker(language=language)

    def check_url(self, url, html_content):
        """Analiza el texto de una URL"""
        soup = BeautifulSoup(html_content, 'html.parser')

        # Extraer texto visible (excluir scripts, styles)
        for script in soup(['script', 'style', 'noscript']):
            script.decompose()

        text = soup.get_text(separator=' ', strip=True)

        results = {
            'total_words': len(text.split()),
            'issues': [],
            'warnings': []
        }

        # 1. Verificar ortograf√≠a
        misspelled = self._check_spelling(text)
        if misspelled:
            results['warnings'].append({
                'type': 'spelling',
                'count': len(misspelled),
                'examples': list(misspelled)[:10]  # Primeras 10
            })

        # 2. Verificar palabras repetidas
        repeated = self._find_repeated_words(text)
        if repeated:
            results['issues'].append({
                'type': 'repeated_words',
                'examples': repeated
            })

        # 3. Verificar texto en MAY√öSCULAS
        all_caps = self._find_all_caps_text(text)
        if all_caps:
            results['warnings'].append({
                'type': 'all_caps',
                'examples': all_caps
            })

        # 4. Verificar legibilidad
        readability_issues = self._check_readability(soup)
        if readability_issues:
            results['warnings'].extend(readability_issues)

        return results

    def _check_spelling(self, text):
        """Busca errores ortogr√°ficos"""
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√±]+\b', text.lower())
        # Filtrar palabras comunes que no est√°n en el diccionario (URLs, n√∫meros, etc.)
        words = [w for w in words if len(w) > 3]

        misspelled = self.spell.unknown(words)
        return misspelled

    def _find_repeated_words(self, text):
        """Busca palabras consecutivas repetidas"""
        pattern = r'\b(\w+)\s+\1\b'
        matches = re.findall(pattern, text, re.IGNORECASE)
        return list(set(matches))

    def _find_all_caps_text(self, text):
        """Busca texto en MAY√öSCULAS (>5 palabras seguidas)"""
        sentences = text.split('.')
        all_caps = []

        for sentence in sentences:
            words = sentence.split()
            caps_words = [w for w in words if w.isupper() and len(w) > 3]
            if len(caps_words) >= 5:
                all_caps.append(' '.join(caps_words[:10]))

        return all_caps

    def _check_readability(self, soup):
        """Verifica problemas de legibilidad"""
        issues = []

        # Buscar p√°rrafos muy largos (>500 palabras sin <br> o <p>)
        paragraphs = soup.find_all(['p', 'div'])
        for p in paragraphs:
            text = p.get_text(strip=True)
            word_count = len(text.split())

            if word_count > 500:
                issues.append({
                    'type': 'long_paragraph',
                    'word_count': word_count,
                    'excerpt': text[:100] + '...'
                })

        return issues
```

#### Limitaciones y decisiones
- **No usar GPT** (de pago): Usar LanguageTool API gratuita con rate limiting
- **Falsos positivos**: Nombres propios, t√©rminos t√©cnicos - permitir "ignorar palabra"
- **Solo espa√±ol**: Inicialmente solo ES, luego agregar EN si es necesario

**Esfuerzo**: ‚≠ê‚≠ê‚≠ê‚≠ê (Alto - muchos falsos positivos a ajustar)
**Valor**: ‚≠ê‚≠ê‚≠ê‚≠ê (Alto - profesionalismo)

---

### üî≤ Phase 3.4: Verificaci√≥n de Informaci√≥n Actualizada (PRIORIDAD 4)
**Duraci√≥n estimada**: 1 semana
**Valor para el usuario**: ‚≠ê‚≠ê‚≠ê (Medio - depende del sitio)

#### ¬øQu√© detectar?
1. **Fechas antiguas** en el texto (a√±os pasados en copyright, "Actualizado en 2022", etc.)
2. **Cambios en el contenido** desde √∫ltima revisi√≥n
3. **Links a contenido obsoleto** (referencias a productos discontinuados, etc.)
4. **Informaci√≥n contradictoria** entre p√°ginas

#### Herramientas gratuitas
- **difflib** (Python built-in) - detectar cambios en texto
- **Regex** - buscar patrones de fechas
- **BeautifulSoup4** - extraer fechas y contenido

#### Implementaci√≥n
```python
# calidad/contenido.py
import re
from datetime import datetime
from difflib import SequenceMatcher
from bs4 import BeautifulSoup

class ContentChecker:
    """Verificador de contenido actualizado"""

    def check_url(self, url, html_content, previous_content=None):
        """Analiza si el contenido est√° actualizado"""
        soup = BeautifulSoup(html_content, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)

        results = {
            'issues': [],
            'warnings': []
        }

        # 1. Buscar fechas antiguas
        old_dates = self._find_old_dates(text)
        if old_dates:
            results['warnings'].append({
                'type': 'old_dates',
                'dates': old_dates
            })

        # 2. Detectar cambios desde √∫ltima revisi√≥n
        if previous_content:
            changes = self._detect_changes(text, previous_content)
            if changes['similarity'] < 0.95:  # Si cambi√≥ m√°s del 5%
                results['warnings'].append({
                    'type': 'content_changed',
                    'similarity': changes['similarity'],
                    'diff_preview': changes['preview']
                })

        return results

    def _find_old_dates(self, text):
        """Busca fechas antiguas (>2 a√±os)"""
        current_year = datetime.now().year
        threshold_year = current_year - 2

        # Buscar a√±os de 4 d√≠gitos
        years = re.findall(r'\b(19|20)\d{2}\b', text)
        old_years = [y for y in years if int(y) < threshold_year]

        return list(set(old_years))

    def _detect_changes(self, current_text, previous_text):
        """Detecta cambios entre versiones de contenido"""
        similarity = SequenceMatcher(None, previous_text, current_text).ratio()

        # Generar preview de cambios (simplificado)
        if similarity < 0.95:
            preview = f"Content changed {(1-similarity)*100:.1f}%"
        else:
            preview = "No significant changes"

        return {
            'similarity': similarity,
            'preview': preview
        }
```

#### UI - Dashboard de Contenido
- **Vista resumen**: "X p√°ginas con fechas antiguas, Y p√°ginas con cambios"
- **Historial**: Mostrar cu√°ndo cambi√≥ cada p√°gina por √∫ltima vez
- **Diff visual**: Mostrar diferencias entre versi√≥n actual y anterior

**Esfuerzo**: ‚≠ê‚≠ê (Bajo - relativamente simple)
**Valor**: ‚≠ê‚≠ê‚≠ê (Medio - √∫til pero no cr√≠tico)

---

### üî≤ Phase 3.5: Verificaci√≥n de FAQ (PRIORIDAD 5)
**Duraci√≥n estimada**: 1 semana
**Valor para el usuario**: ‚≠ê‚≠ê‚≠ê (Medio - depende si tienen FAQ)

#### ¬øQu√© detectar?
1. **FAQ sin estructura sem√°ntica** (sin `<details>`, `<summary>`, o schema markup)
2. **Preguntas sin respuestas**
3. **Respuestas muy cortas** (<20 palabras)
4. **FAQ desactualizado** (referencias a cosas obsoletas)
5. **FAQ no accesible** (no navegable con teclado)

#### Implementaci√≥n
```python
# calidad/faq.py
from bs4 import BeautifulSoup

class FAQChecker:
    """Verificador de FAQ (Frequently Asked Questions)"""

    def check_url(self, url, html_content):
        """Analiza la calidad del FAQ"""
        soup = BeautifulSoup(html_content, 'html.parser')

        results = {
            'has_faq': False,
            'issues': [],
            'warnings': []
        }

        # Detectar si la p√°gina tiene FAQ
        faq_indicators = [
            soup.find('section', class_=re.compile('faq', re.I)),
            soup.find('div', id=re.compile('faq', re.I)),
            soup.find_all('details'),  # HTML5 FAQ structure
        ]

        if any(faq_indicators):
            results['has_faq'] = True

            # Verificar estructura sem√°ntica
            details_tags = soup.find_all('details')
            if not details_tags:
                results['warnings'].append({
                    'type': 'no_semantic_structure',
                    'message': 'FAQ without <details>/<summary> structure'
                })

            # Verificar schema markup (structured data para SEO)
            script_ld = soup.find('script', type='application/ld+json')
            if not script_ld or 'FAQPage' not in str(script_ld):
                results['warnings'].append({
                    'type': 'no_schema_markup',
                    'message': 'FAQ without FAQPage schema.org markup (SEO)'
                })

        return results
```

**Esfuerzo**: ‚≠ê‚≠ê (Bajo)
**Valor**: ‚≠ê‚≠ê‚≠ê (Medio - solo si tienen FAQ)

---

### üî≤ Phase 3.6: Verificaci√≥n de Dise√±o y Accesibilidad (PRIORIDAD 6)
**Duraci√≥n estimada**: 2 semanas
**Valor para el usuario**: ‚≠ê‚≠ê‚≠ê‚≠ê (Alto - legal y UX)

#### ¬øQu√© detectar?
1. **Contraste insuficiente** (WCAG AA compliance)
2. **Texto demasiado peque√±o** (<16px)
3. **Links sin indicaci√≥n de visitado**
4. **Formularios sin labels**
5. **Im√°genes sin role o aria-label**
6. **Problemas responsive** (viewport, breakpoints)

#### Herramientas gratuitas
- **wcag-contrast-ratio** (Python) - calcular contraste de colores
- **BeautifulSoup4** - analizar estructura HTML
- **Lighthouse CLI** (opcional, via subprocess) - auditor√≠a completa

#### Implementaci√≥n (simplificada)
```python
# calidad/diseno.py
from bs4 import BeautifulSoup
import re

class DesignChecker:
    """Verificador de dise√±o y accesibilidad"""

    MIN_CONTRAST_RATIO = 4.5  # WCAG AA
    MIN_FONT_SIZE = 16

    def check_url(self, url, html_content):
        """Analiza accesibilidad y dise√±o"""
        soup = BeautifulSoup(html_content, 'html.parser')

        results = {
            'issues': [],
            'warnings': []
        }

        # 1. Verificar formularios sin labels
        forms = soup.find_all('form')
        for form in forms:
            inputs = form.find_all('input')
            for inp in inputs:
                if inp.get('type') not in ['submit', 'button', 'hidden']:
                    label = form.find('label', {'for': inp.get('id')})
                    if not label and not inp.get('aria-label'):
                        results['issues'].append({
                            'type': 'input_without_label',
                            'input_type': inp.get('type', 'text')
                        })

        # 2. Verificar viewport meta tag
        viewport = soup.find('meta', {'name': 'viewport'})
        if not viewport:
            results['warnings'].append({
                'type': 'missing_viewport',
                'message': 'No viewport meta tag (responsive issue)'
            })

        # 3. Verificar im√°genes decorativas sin role
        images = soup.find_all('img')
        for img in images:
            alt = img.get('alt', '')
            role = img.get('role', '')

            if not alt and not role:
                results['warnings'].append({
                    'type': 'decorative_image_no_role',
                    'src': img.get('src', '')[:50]
                })

        return results
```

**Esfuerzo**: ‚≠ê‚≠ê‚≠ê‚≠ê (Alto - muchos aspectos a verificar)
**Valor**: ‚≠ê‚≠ê‚≠ê‚≠ê (Alto - cumplimiento legal WCAG)

---

## üìä Dashboard Final (Stage 3 Completo)

### Vista Unificada de Calidad
```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë              DASHBOARD DE CALIDAD WEB - 173 URLs            ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                              ‚ïë
‚ïë  üìä ESTADO GENERAL                    üïê √öltima revisi√≥n:   ‚ïë
‚ïë  ‚úÖ 156 URLs OK (90%)                    31/10/2025 14:35   ‚ïë
‚ïë  ‚ö†Ô∏è  12 URLs con warnings (7%)                              ‚ïë
‚ïë  ‚ùå 5 URLs con issues cr√≠ticos (3%)                         ‚ïë
‚ïë                                                              ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  üîç VERIFICACIONES AUTOM√ÅTICAS (8)                           ‚ïë
‚ïë                                                              ‚ïë
‚ïë  1. üîó Enlaces             ‚úÖ 98% OK   ‚ùå 3 rotos            ‚ïë
‚ïë  2. üñºÔ∏è  Im√°genes            ‚ö†Ô∏è  82% OK   ‚ö†Ô∏è  12 sin alt      ‚ïë
‚ïë  3. üéØ CTAs                ‚úÖ 95% OK   ‚ö†Ô∏è  2 gen√©ricos       ‚ïë
‚ïë  4. üìù Textos              ‚ö†Ô∏è  88% OK   ‚ö†Ô∏è  8 con erratas    ‚ïë
‚ïë  5. üîÑ Contenido           ‚úÖ 100% OK  ‚úÖ Todo actualizado  ‚ïë
‚ïë  6. ‚ùì FAQ                 ‚úÖ 100% OK  ‚úÖ Bien estructurado ‚ïë
‚ïë  7. üé® Dise√±o              ‚ö†Ô∏è  90% OK   ‚ö†Ô∏è  5 sin labels     ‚ïë
‚ïë  8. üì± Responsive          ‚úÖ 100% OK  ‚úÖ Viewport OK       ‚ïë
‚ïë                                                              ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  üö® ACCIONES REQUERIDAS                                      ‚ïë
‚ïë                                                              ‚ïë
‚ïë  ‚Ä¢ 3 enlaces rotos - [Ver detalles] [Crear tarea]          ‚ïë
‚ïë  ‚Ä¢ 12 im√°genes sin alt text - [Revisar] [Asignar]          ‚ïë
‚ïë  ‚Ä¢ 8 p√°ginas con erratas - [Corregir] [Ignorar]            ‚ïë
‚ïë                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

### Flujo de Trabajo con Notificaciones
1. **Crawler ejecuta verificaciones** (programado diariamente a las 3 AM)
2. **Sistema genera resumen** de nuevos issues
3. **Email autom√°tico** solo si hay issues cr√≠ticos o nuevos warnings
4. **Tu esposa revisa dashboard** cuando le llegue notificaci√≥n
5. **Marca como "revisado"** o crea tarea manual si requiere acci√≥n humana

---

## üõ†Ô∏è Dependencias Nuevas

```txt
# requirements.txt - Agregar:
Pillow==10.4.0              # Phase 3.1 - Im√°genes
imagehash==4.3.1            # Phase 3.1 - Duplicados (opcional)
pyspellchecker==0.8.1       # Phase 3.3 - Ortograf√≠a
wcag-contrast-ratio==0.9    # Phase 3.6 - Accesibilidad
```

**TOTAL**: 4 librer√≠as nuevas (todas gratuitas, sin APIs externas)

---

## üìà Cronograma Estimado (3-4 meses)

| Fase | Descripci√≥n | Duraci√≥n | Fechas Aprox. |
|------|-------------|----------|---------------|
| **3.0** | Refactoring + arquitectura modular | 1 semana | 01-08 Nov |
| **MANTENIMIENTO** | Revisar c√≥digo, optimizar | 2 d√≠as | 09-10 Nov |
| **3.1** | Verificaci√≥n de Im√°genes | 2 semanas | 11-24 Nov |
| **MANTENIMIENTO** | Revisar c√≥digo, tests | 2 d√≠as | 25-26 Nov |
| **3.2** | Verificaci√≥n de CTAs | 1.5 semanas | 27 Nov - 07 Dic |
| **MANTENIMIENTO** | Revisar c√≥digo, tests | 2 d√≠as | 08-09 Dic |
| **3.3** | Verificaci√≥n de Textos | 2 semanas | 10-23 Dic |
| **PAUSA NAVIDE√ëA** | -- | 1 semana | 24-31 Dic |
| **MANTENIMIENTO** | Revisar c√≥digo, tests | 2 d√≠as | 01-02 Ene |
| **3.4** | Verificaci√≥n de Contenido | 1 semana | 03-09 Ene |
| **MANTENIMIENTO** | Revisar c√≥digo, tests | 2 d√≠as | 10-11 Ene |
| **3.5** | Verificaci√≥n de FAQ | 1 semana | 12-18 Ene |
| **MANTENIMIENTO** | Revisar c√≥digo, tests | 2 d√≠as | 19-20 Ene |
| **3.6** | Verificaci√≥n de Dise√±o | 2 semanas | 21 Ene - 03 Feb |
| **TESTING FINAL** | Testing integral, bugs | 1 semana | 04-10 Feb |
| **DOCUMENTACI√ìN** | Portfolio, docs | 3 d√≠as | 11-13 Feb |

**TOTAL**: ~14 semanas = 3.5 meses

---

## üéØ Criterios de √âxito

### Para tu esposa (usuario final)
- ‚úÖ Reducir tiempo de revisi√≥n manual de **8 horas/semana ‚Üí 1 hora/semana**
- ‚úÖ Eliminar el Excel completamente
- ‚úÖ Recibir solo notificaciones de issues reales (no falsos positivos)
- ‚úÖ Dashboard claro y f√°cil de entender

### Para ti (desarrollador/portfolio)
- ‚úÖ C√≥digo modular, mantenible, documentado
- ‚úÖ Sistema escalable (f√°cil agregar m√°s verificaciones)
- ‚úÖ Zero downtime en producci√≥n
- ‚úÖ Demo funcional para portfolio (screenshots, video)
- ‚úÖ Arquitectura demostrable en entrevistas

### M√©tricas t√©cnicas
- ‚úÖ `app.py` < 1,000 l√≠neas (actualmente 1,647)
- ‚úÖ Cobertura de tests > 70%
- ‚úÖ Tiempo de crawl completo < 30 minutos (173 URLs)
- ‚úÖ Tasa de falsos positivos < 5%

---

## üö´ Restricciones y No-Hacer

### NO implementar
- ‚ùå **Machine Learning** - Overkill para 173 URLs, no gratuito (compute)
- ‚ùå **An√°lisis de video/multimedia** - Fuera de scope
- ‚ùå **An√°lisis de performance detallado** (Lighthouse completo) - Demasiado lento
- ‚ùå **Integraci√≥n con CMS** - No es objetivo actual
- ‚ùå **Multi-tenant** - Solo para una empresa por ahora
- ‚ùå **APIs de terceros de pago** - Solo herramientas gratuitas

### S√ç mantener
- ‚úÖ **Simplicidad**: Soluciones simples antes que complejas
- ‚úÖ **Incremental**: Una verificaci√≥n a la vez
- ‚úÖ **Testing real**: Probar en 10-20 URLs antes de escalar
- ‚úÖ **Portfolio-ready**: C√≥digo limpio, commits descriptivos
- ‚úÖ **Documentaci√≥n continua**: README actualizado, comentarios √∫tiles

---

## üìö Recursos y Referencias

### Documentaci√≥n t√©cnica
- [WCAG Guidelines](https://www.w3.org/WAI/WCAG21/quickref/)
- [Schema.org FAQPage](https://schema.org/FAQPage)
- [Google's Web Vitals](https://web.dev/vitals/)

### Herramientas gratuitas usadas
- **BeautifulSoup4**: Web scraping
- **Pillow**: Procesamiento de im√°genes
- **pyspellchecker**: Correcci√≥n ortogr√°fica offline
- **LanguageTool API**: Gram√°tica (rate limited gratis)
- **wcag-contrast-ratio**: Accesibilidad

### Alternativas evaluadas y descartadas
- ‚ùå **Selenium** - Demasiado lento para 173 URLs
- ‚ùå **Scrapy** - Overkill, preferimos simplicidad
- ‚ùå **GPT-4 API** - No es gratuito
- ‚ùå **AWS/GCP services** - No es gratuito

---

## üîÑ Estrategia de Rollback

Si una fase falla o genera demasiados falsos positivos:

1. **Desactivar verificaci√≥n** en scheduler (flag en DB)
2. **Mantener c√≥digo** pero no ejecutar autom√°ticamente
3. **Permitir ejecuci√≥n manual** para debugging
4. **Iterar** hasta reducir falsos positivos
5. **Reactivar** cuando est√© listo

**No eliminar c√≥digo** - siempre puede ser √∫til m√°s adelante.

---

## ‚úÖ Checklist de Inicio (Antes de Phase 3.0)

- [ ] Hacer backup de base de datos actual
- [ ] Crear branch `stage3-development`
- [ ] Actualizar `.claude/01-current-phase.md` con Phase 3.0
- [ ] Leer `MAINTENANCE_CHECKLIST.md` (pr√≥ximo documento)
- [ ] Confirmar que Stage 2 est√° 100% funcional en producci√≥n
- [ ] Instalar dependencias nuevas (`pip install Pillow pyspellchecker`)
- [ ] Crear estructura de carpetas `calidad/`

---

**Documento creado**: 2025-10-31
**√öltima actualizaci√≥n**: 2025-10-31
**Autor**: Claude Code + Jes√∫s Ramos
**Estado**: PLAN - Pendiente de aprobaci√≥n

---

## üí¨ Preguntas Frecuentes

**P: ¬øPor qu√© no usar Selenium para JavaScript?**
R: La mayor√≠a de las verificaciones (im√°genes, textos, CTAs) no requieren JavaScript ejecutado. Si m√°s adelante detectamos que hay contenido din√°mico cr√≠tico, podemos agregar Playwright (m√°s r√°pido que Selenium) solo para esas URLs espec√≠ficas.

**P: ¬øQu√© pasa si hay demasiados falsos positivos?**
R: Cada verificaci√≥n tiene un sistema de "ignorar" para palabras/elementos espec√≠ficos. Adem√°s, el dashboard permite marcar issues como "falso positivo" para que no vuelvan a aparecer.

**P: ¬øC√≥mo se integran estas verificaciones con el crawler existente?**
R: El crawler existente (Stage 2) descubre URLs y verifica enlaces. Las nuevas verificaciones (Stage 3) se ejecutan sobre las URLs ya descubiertas, como plugins modulares. No modifican el crawler base.

**P: ¬øEsto reemplaza herramientas como Lighthouse/PageSpeed?**
R: No, esto es complementario. Lighthouse hace an√°lisis muy profundos pero es lento (30s por p√°gina = 1.5 horas para 173 URLs). Nuestro sistema hace verificaciones m√°s espec√≠ficas y r√°pidas (~5-10s por p√°gina = 15-30 minutos total).

**P: ¬øQu√© pasa con el Excel actual?**
R: Una vez completado Stage 3, el Excel se vuelve obsoleto. Todas las verificaciones se hacen autom√°ticamente y el sistema notifica solo cuando hay problemas. El trabajo manual se reduce de 8h/semana a ~1h/semana (solo revisar issues detectados).
